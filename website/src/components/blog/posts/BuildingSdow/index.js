import React from 'react';
import {Helmet} from 'react-helmet';

import StyledLink from '../../../common/StyledLink';
import NewsletterSignupForm from '../../NewsletterSignupForm';

import {
  P,
  UL,
  Code,
  Image,
  Title,
  Divider,
  Subtitle,
  SectionTitle,
  Stat,
  StatsWrapper,
  Wrapper,
} from '../../BlogPost/index.styles';

import architecturalDiagramImage from './architecturalDiagram.png';

const title = 'Building Six Degrees of Wikipedia';

export default () => (
  <Wrapper>
    <Helmet>
      <title>{`${title} | Blog | Six Degrees of Wikipedia`}</title>
    </Helmet>

    <Title>{title}</Title>
    <Subtitle>
      <StyledLink href="https://jwn.gr">Jacob Wenger</StyledLink> | March 22, 2018
    </Subtitle>

    <P>
      Building <StyledLink href="/">Six Degrees of Wikipedia</StyledLink> was a learning experience
      for me. Building a live service from scratch can be a lot of work. It is often not clear what
      programming languages or technologies to use in order to deliver your vision on a budget and
      in a timely manner. While I spent more money and time than expected bringing the project to
      life, I learned a lot in doing so. I want to dive into how I built this project and lessons
      learned along the way.
    </P>

    <SectionTitle>Architectural Overview</SectionTitle>

    <P>
      The easiest place to start is to step back and look at a high level architectural overview of
      the project:
    </P>

    <Image src={architecturalDiagramImage} alt="Six Degrees of Wikipedia architectural diagram" />

    <P>
      Green pieces are things written or generated by me. Red pieces are either infrastructure or
      files generated by other people. The flow through the project is generally left to right. A
      database generation script written in Python and run on a beefy GCE VM downloads a dump of the
      Wikipedia database and generates the Six Degrees of Wikipedia SQLite database. A web server
      written in Flask, a Python web microframework, uses the generated SQLite file to run the
      search and fulfill requests from the website built in React and hosted on Firebase Hosting.
      Both the website frontend and server backend make use of the Mediawiki API to fetch page
      suggestions and page details such as image URLs and descriptions, respectively.
    </P>

    <P>Let's dive into more specifics and lessons learned from different pieces of this diagram.</P>

    <SectionTitle>Database Generation Script</SectionTitle>

    <P>
      Wikimedia generates freely available{' '}
      <StyledLink href="https://dumps.wikimedia.your.org/enwiki">gzipped SQL dumps</StyledLink> of
      the English language Wikipedia database twice a month. All I needed to find the shortest paths
      between pages were the following three tables:
    </P>

    <UL>
      <li>
        <Code>page</Code> - Contains the ID and name (among other things) for all pages.
      </li>
      <li>
        <Code>pagelinks</Code> - Contains the source and target pages all links.
      </li>
      <li>
        <Code>redirects</Code> - Contains the source and target pages for all redirects.
      </li>
    </UL>

    <P>
      For performance reasons, files are downloaded from the <Code>dumps.wikimedia.your.org</Code>
      mirror and I only concern myself with actual Wikipedia pages, which in Wikipedia parlance
      means those pages belonging to{' '}
      <a href="https://en.wikipedia.org/wiki/Wikipedia:Namespace">namespace</a> <Code>0</Code>.
    </P>

    <P>
      It was challenging to efficiently process such large files. My first idea was to import the
      raw SQL dumps into my own local SQL database and create a new, trimmed down database from it
      by querying only the columns I needed. Hours into the SQL import, I realized this was not a
      scalable approach. So I switched tracks and decided to parse the data I needed from the raw
      SQL dumps into CSV (comma-separate values) files. After discovering many page titles
      themselves have commas in them, I switched to using TSV (tab-separated values) files.
    </P>

    <P>
      I used a series of Pythons scripts to parse the raw SQL dumps. Despite my best attempts at
      using streaming libraries and efficient data structures, the Python scripts ran into both CPU
      and memory issues and took hours to run. I went back to the drawing board and went with
      built-in UNIX commands like <Code>sed</Code>, <Code>awk</Code>, <Code>grep</Code>, and{' '}
      <Code>sort</Code> worked beautifully and more reliably. My biggest frustration with this
      approach is that I still need to read and re-read the documentation for each command's
      arguments every time I go to update them and there were some non-obvious performance tuning
      that drastically changed run time. There were certain things which were simply more
      understandable to do in a Python script compared to a long, complex, indecipherable
      command-line utility, but in general, the database generation script.
    </P>

    <P>
      <b>Lesson Learned:</b> When dealing with large amounts of data, it is often more efficient to
      rely on built-in UNIX commands which are designed specifically for your use case and don't
      have the overhead of a programming language.
    </P>

    <P>
      <b>Lesson Learned:</b> Many of the built-in UNIX commands have counterparts which are more
      efficient given their smaller use case. Explore your options
    </P>

    <P>
      Given the sheer size of the raw SQL dumps, I need to run the database generation script on a
      beefy GCM VM.
    </P>

    <P>Why SQLite?</P>

    <P>Storing stuff on GCS</P>

    <P>
      Ultimately, the current database generation script takes about an hour to download, trim, and
      parse the latest dump from Wikipedia into the Six Degrees of Wikipedia SQLite database file,
      costing me about 40 cents. Going forward, I'll run it at the beginning of every month.
    </P>

    <SectionTitle>Building The Website</SectionTitle>

    <P>
      The JavaScript ecosystem is continuing to evolve rapidly. While tools like React, Babel, and
      webpack have made website development easier and more enjoyable for seasoned developers, it is
      sometimes frustrating live on the bleeding edge. After one too many hours spent debugging a
      custom configuration issue, I vowed to slow down and find a front-end stack which was more
      reliable and in which I was more productive.{' '}
      <StyledLink href="https://github.com/facebook/create-react-app">Create React App</StyledLink>{' '}
      has been that thing for me. It gets me 90% of the features I had in my custom setup at a
      fraction of the setup and maintenance costs. It comes with smart defaults for Babel and
      webpack out-of-the-box and is flexible enough that things like ESLint, Prettier, and{' '}
      <Code>styled-components</Code> are easy to add without managing ever-growing configuration
      files.
    </P>

    <P>
      <b>Lesson Learned:</b> As tempting as that shiny new framework or library may be, you're more
      likely to actually ship your side project if you choose a stack which allows you to focus on
      the problem at hand instead of configuring your development environment.
    </P>

    <P>And don't get me started on how enjoyable it is to work with React.</P>

    <SectionTitle>Creating The Server Backend</SectionTitle>

    <P>
      Python seemed like a good fit for this project given my past experience with the language. I
      am huge fan of its syntax and knew it could handle the size of the data I'd be sending through
      it. When it came to choosing a web framework, I knew I didn't need anything nearly as
      full-featured as Django and I had been meaning to give{' '}
      <StyledLink href="http://flask.pocoo.org/">Flask</StyledLink> a try. It bills itself as a
      Python microframework and given it's good API and documentation was not to hard to learn.
    </P>

    <P>
      The search algorithm itself is a bi-directional breadth first search. As the number of degrees
      of separation grows in the search, the number of pages visited grows rapidly. I ended up
      pre-computing the incoming and outgoing links for all pages and storing that in my database in
      order to make those searches faster. It took longer in the database generation script, but
      that was a one-time, offline cost which made the actual searches much quicker.
    </P>

    <P>
      <b>Lesson Learned:</b> If you are concerned about performance, it is often a good tradeoff to
      take a big one-time cost upfront instead of a smaller cost continually, especially if that
      upfront cost is completely hidden to end users.
    </P>

    <P>
      The search algorithm itself is a recursive bi-directional breadth first search. As the number
      of degrees of separation grows in the search, the number of pages visited grows rapidly. I
      ended up pre-computing the incoming and outgoing links for all pages and storing that in my
      database in order to make those searches faster. It took longer in the database generation
      script, but that was a one-time, offline cost which made the actual searches much quicker.
    </P>

    <SectionTitle>Onwards To Production</SectionTitle>

    <P>
      It was one thing to get this project running locally and entirely other thing to have it
      running in production. Certain aspects of this were easier than others.
    </P>

    <P>
      I used{' '}
      <StyledLink href="https://firebase.google.com/docs/hosting/">Firebase Hosting</StyledLink> to
      connect my custom domain and host the website. Although as a former Firebase engineer I am
      certainly biased, Firebase Hosting is such an easy and powerful product to use that I cannot
      really imagine how it could be easier. I even set up{' '}
      <StyledLink href="https://semaphoreci.com/">Semaphore CI</StyledLink> to auto-deploy the site
      every time I pushed changes to GitHub using{' '}
      <StyledLink href="https://github.com/firebase/firebase-tools#using-with-ci-systems">
        these instructions
      </StyledLink>.
    </P>

    <P>
      Getting the Flask web server running in production was a more difficult task. I needed a
      system which allowed for handling my large SQLite database file (coming it at around 8 GB) and
      I didn't want to have to deal with managing my own hardware. Google Compute Engine fit the
      bill.
    </P>

    <P>
      The first problem I ran into was HTTPS. Firebase Hosting serves everything over HTTPS, meaning
      all outgoing network requests from my site also needed to be HTTPS. At first, I just got my
      Flask app running on my GCE instance by exposing the Flask HTTP port in my VM's firewall. This
      allowed me to hit the URL directly and from my local instance of my website, but not from
      Firebase Hosting.
    </P>

    <P>
      As my planned launch day drew closer, I knew I needed to solve my HTTPS woes. I bit the bullet
      and dove into how to configure a Let's Encrypt SSL certificate on my machine. I acquired a
      static IP address from Google Cloud and added an <Code>A</Code> record to my website's DNS to
      point the <Code>api</Code> subdomain to point to it. Then, it was a matter of installing Nginx
      and{' '}
      <StyledLink href="https://github.com/jwngr/sdow/blob/master/docs/web-server-setup.md#initial-setup">
        following a handful of steps
      </StyledLink>{' '}
      to verify ownership of the domain.
    </P>

    <Divider />

    <NewsletterSignupForm />
  </Wrapper>
);
